# @package _global_
defaults:
  - override /model: small
  - override /transforms: baseline
  - override /scheduler: null

experience_name: dev

data:
  kwargs:
    batch_size: 64
    num_positives:
      train: 'random'
      test: 'all'

transforms:
  image:
    train_resize_size: 36
    train_resolution: 32
    test_resize_size: 36
    test_resolution: 36

loss:
  margin: 0.2
  hardest_fraction: 1.0

trainer:
  kwargs:
    max_epochs: 75
  callbacks: 
    - name: HardestFractionDecay
      kwargs:
        total_steps: 5000
        min_fraction: 0.9
        schedule: hyperbolic
        k: 16

optimizer:
  name: AdamW
  kwargs:
    lr: 1e-3
    betas: [0.9, 0.999]
    weight_decay: 0

scheduler:
  - name: LinearLRWarmup
    kwargs:
      num_warmup_steps: 60
    pl_kwargs:
      interval: step
  - name: MultiStepLR
    kwargs:
      milestones: [25]
      gamma: 0.1
    pl_kwargs:
      interval: epoch
